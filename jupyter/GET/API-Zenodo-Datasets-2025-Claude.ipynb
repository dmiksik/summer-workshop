{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "220af052-aa7e-4a53-bba1-a29708f04ce2",
   "metadata": {},
   "source": [
    "# Stažení záznamů o všech datasetech v Zenodu za r. 2025\n",
    "\n",
    "## Základní parametry dotazu:\n",
    "* `'q': 'resource_type.type:dataset AND publication_date:2025'`\n",
    "* `'size': page_size,`\n",
    "* `'page': page,`\n",
    "* `'sort': 'mostrecent'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79754943-637e-41dc-909e-36178e5ef12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "\n",
    "class ZenodoDatasetRetriever:\n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://zenodo.org/api/records\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'ZenodoDatasetRetriever/1.0'\n",
    "        })\n",
    "        \n",
    "    def get_datasets_2025(self, page_size: int = 100) -> List[Dict[Any, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve all dataset records from Zenodo deposited in 2025\n",
    "        \"\"\"\n",
    "        all_records = []\n",
    "        page = 1\n",
    "        total_hits = None\n",
    "        \n",
    "        while True:\n",
    "            print(f\"Fetching page {page}...\")\n",
    "            \n",
    "            # Parameters for the API request\n",
    "            params = {\n",
    "                'q': 'resource_type.type:dataset AND publication_date:2025',\n",
    "                'size': page_size,\n",
    "                'page': page,\n",
    "                'sort': 'mostrecent'\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                response = self.session.get(self.base_url, params=params)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                hits = data.get('hits', {}).get('hits', [])\n",
    "                \n",
    "                # Print total hits on first page\n",
    "                if page == 1:\n",
    "                    total_hits = data.get('hits', {}).get('total', 0)\n",
    "                    print(f\"Total hits found: {total_hits}\")\n",
    "                \n",
    "                if not hits:\n",
    "                    print(f\"No more records found. Total pages processed: {page-1}\")\n",
    "                    break\n",
    "                \n",
    "                all_records.extend(hits)\n",
    "                print(f\"Retrieved {len(hits)} records from page {page}. Total so far: {len(all_records)}\")\n",
    "                \n",
    "                # Check if we've reached the last page\n",
    "                if len(all_records) >= total_hits:\n",
    "                    print(f\"Retrieved all {total_hits} records\")\n",
    "                    break\n",
    "                \n",
    "                page += 1\n",
    "                \n",
    "                # Be respectful to the API - add a small delay\n",
    "                time.sleep(0.1)\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching page {page}: {e}\")\n",
    "                break\n",
    "                \n",
    "        return all_records\n",
    "    \n",
    "    def extract_record_info(self, record: Dict[Any, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract relevant information from a Zenodo record\n",
    "        \"\"\"\n",
    "        metadata = record.get('metadata', {})\n",
    "        \n",
    "        # Extract creators\n",
    "        creators = metadata.get('creators', [])\n",
    "        creator_names = [creator.get('name', '') for creator in creators]\n",
    "        \n",
    "        # Extract keywords\n",
    "        keywords = metadata.get('keywords', [])\n",
    "        \n",
    "        # Extract related identifiers\n",
    "        related_identifiers = metadata.get('related_identifiers', [])\n",
    "        \n",
    "        # Extract file information\n",
    "        files = record.get('files', [])\n",
    "        file_info = []\n",
    "        total_size = 0\n",
    "        \n",
    "        for file in files:\n",
    "            file_info.append({\n",
    "                'filename': file.get('key', ''),\n",
    "                'size': file.get('size', 0),\n",
    "                'checksum': file.get('checksum', '')\n",
    "            })\n",
    "            total_size += file.get('size', 0)\n",
    "        \n",
    "        return {\n",
    "            'id': record.get('id', ''),\n",
    "            'doi': record.get('doi', ''),\n",
    "            'title': metadata.get('title', ''),\n",
    "            'description': metadata.get('description', ''),\n",
    "            'publication_date': metadata.get('publication_date', ''),\n",
    "            'creators': '; '.join(creator_names),\n",
    "            'keywords': '; '.join(keywords),\n",
    "            'license': metadata.get('license', {}).get('id', ''),\n",
    "            'access_right': metadata.get('access_right', ''),\n",
    "            'resource_type': metadata.get('resource_type', {}).get('type', ''),\n",
    "            'language': metadata.get('language', ''),\n",
    "            'version': metadata.get('version', ''),\n",
    "            'file_count': len(files),\n",
    "            'total_size_bytes': total_size,\n",
    "            'total_size_mb': round(total_size / (1024 * 1024), 2),\n",
    "            'zenodo_url': f\"https://zenodo.org/record/{record.get('id', '')}\",\n",
    "            'created': record.get('created', ''),\n",
    "            'updated': record.get('updated', ''),\n",
    "            'conceptrecid': record.get('conceptrecid', ''),\n",
    "            'conceptdoi': record.get('conceptdoi', ''),\n",
    "            'related_identifiers_count': len(related_identifiers),\n",
    "            'bucket_url': record.get('links', {}).get('bucket', ''),\n",
    "            'files_info': json.dumps(file_info) if file_info else ''\n",
    "        }\n",
    "    \n",
    "    def extract_affiliations(self, records: List[Dict[Any, Any]]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract all affiliations from records and count their occurrences\n",
    "        \"\"\"\n",
    "        affiliations = []\n",
    "        \n",
    "        for record in records:\n",
    "            metadata = record.get('metadata', {})\n",
    "            \n",
    "            # Check creators for affiliations\n",
    "            creators = metadata.get('creators', [])\n",
    "            for creator in creators:\n",
    "                if isinstance(creator, dict) and 'affiliation' in creator:\n",
    "                    affiliation = creator['affiliation']\n",
    "                    if affiliation and affiliation.strip():\n",
    "                        affiliations.append(affiliation.strip())\n",
    "            \n",
    "            # Check contributors for affiliations\n",
    "            contributors = metadata.get('contributors', [])\n",
    "            for contributor in contributors:\n",
    "                if isinstance(contributor, dict) and 'affiliation' in contributor:\n",
    "                    affiliation = contributor['affiliation']\n",
    "                    if affiliation and affiliation.strip():\n",
    "                        affiliations.append(affiliation.strip())\n",
    "        \n",
    "        # Count occurrences\n",
    "        if affiliations:\n",
    "            affiliation_counts = pd.Series(affiliations).value_counts()\n",
    "            affiliation_df = pd.DataFrame({\n",
    "                'instituce': affiliation_counts.index,\n",
    "                'počet_výskytů': affiliation_counts.values\n",
    "            })\n",
    "        else:\n",
    "            affiliation_df = pd.DataFrame(columns=['instituce', 'počet_výskytů'])\n",
    "        \n",
    "        return affiliation_df\n",
    "        \"\"\"\n",
    "        Extract relevant information from a Zenodo record\n",
    "        \"\"\"\n",
    "        metadata = record.get('metadata', {})\n",
    "        \n",
    "        # Extract creators\n",
    "        creators = metadata.get('creators', [])\n",
    "        creator_names = [creator.get('name', '') for creator in creators]\n",
    "        \n",
    "        # Extract keywords\n",
    "        keywords = metadata.get('keywords', [])\n",
    "        \n",
    "        # Extract related identifiers\n",
    "        related_identifiers = metadata.get('related_identifiers', [])\n",
    "        \n",
    "        # Extract file information\n",
    "        files = record.get('files', [])\n",
    "        file_info = []\n",
    "        total_size = 0\n",
    "        \n",
    "        for file in files:\n",
    "            file_info.append({\n",
    "                'filename': file.get('key', ''),\n",
    "                'size': file.get('size', 0),\n",
    "                'checksum': file.get('checksum', '')\n",
    "            })\n",
    "            total_size += file.get('size', 0)\n",
    "        \n",
    "        return {\n",
    "            'id': record.get('id', ''),\n",
    "            'doi': record.get('doi', ''),\n",
    "            'title': metadata.get('title', ''),\n",
    "            'description': metadata.get('description', ''),\n",
    "            'publication_date': metadata.get('publication_date', ''),\n",
    "            'creators': '; '.join(creator_names),\n",
    "            'keywords': '; '.join(keywords),\n",
    "            'license': metadata.get('license', {}).get('id', ''),\n",
    "            'access_right': metadata.get('access_right', ''),\n",
    "            'resource_type': metadata.get('resource_type', {}).get('type', ''),\n",
    "            'language': metadata.get('language', ''),\n",
    "            'version': metadata.get('version', ''),\n",
    "            'file_count': len(files),\n",
    "            'total_size_bytes': total_size,\n",
    "            'total_size_mb': round(total_size / (1024 * 1024), 2),\n",
    "            'zenodo_url': f\"https://zenodo.org/record/{record.get('id', '')}\",\n",
    "            'created': record.get('created', ''),\n",
    "            'updated': record.get('updated', ''),\n",
    "            'conceptrecid': record.get('conceptrecid', ''),\n",
    "            'conceptdoi': record.get('conceptdoi', ''),\n",
    "            'related_identifiers_count': len(related_identifiers),\n",
    "            'bucket_url': record.get('links', {}).get('bucket', ''),\n",
    "            'files_info': json.dumps(file_info) if file_info else ''\n",
    "        }\n",
    "    \n",
    "    def save_to_files(self, records: List[Dict[Any, Any]], base_filename: str = 'zenodo_datasets_2025'):\n",
    "        \"\"\"\n",
    "        Save records to CSV, XLS, and JSON formats\n",
    "        \"\"\"\n",
    "        # Process records to extract structured information\n",
    "        processed_records = []\n",
    "        \n",
    "        print(\"Processing records...\")\n",
    "        for i, record in enumerate(records, 1):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processed {i}/{len(records)} records\")\n",
    "            \n",
    "            processed_record = self.extract_record_info(record)\n",
    "            processed_records.append(processed_record)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(processed_records)\n",
    "        \n",
    "        # Extract affiliations\n",
    "        print(\"Extracting affiliations...\")\n",
    "        affiliations_df = self.extract_affiliations(records)\n",
    "        \n",
    "        # Generate filenames with timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"{base_filename}_{timestamp}.csv\"\n",
    "        xlsx_filename = f\"{base_filename}_{timestamp}.xlsx\"\n",
    "        json_filename = f\"{base_filename}_{timestamp}.json\"\n",
    "        affiliations_filename = f\"{base_filename}_affiliations_{timestamp}.csv\"\n",
    "        \n",
    "        # Save raw JSON data\n",
    "        print(f\"Saving to JSON: {json_filename}\")\n",
    "        with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'metadata': {\n",
    "                    'total_records': len(records),\n",
    "                    'retrieval_date': datetime.now().isoformat(),\n",
    "                    'query': 'resource_type.type:dataset AND publication_date:2025',\n",
    "                    'source': 'Zenodo API'\n",
    "                },\n",
    "                'records': records\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # Save to CSV\n",
    "        print(f\"Saving to CSV: {csv_filename}\")\n",
    "        df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Save affiliations to CSV\n",
    "        print(f\"Saving affiliations to CSV: {affiliations_filename}\")\n",
    "        affiliations_df.to_csv(affiliations_filename, index=False, encoding='utf-8')\n",
    "        \n",
    "        # Save to Excel with multiple sheets\n",
    "        print(f\"Saving to Excel: {xlsx_filename}\")\n",
    "        with pd.ExcelWriter(xlsx_filename, engine='openpyxl') as writer:\n",
    "            # Main data sheet\n",
    "            df.to_excel(writer, sheet_name='Datasets', index=False)\n",
    "            \n",
    "            # Affiliations sheet\n",
    "            affiliations_df.to_excel(writer, sheet_name='Affiliations', index=False)\n",
    "            \n",
    "            # Summary sheet\n",
    "            summary_data = {\n",
    "                'Metric': [\n",
    "                    'Total Datasets',\n",
    "                    'Date Range',\n",
    "                    'Most Common License',\n",
    "                    'Most Common Language',\n",
    "                    'Total Size (GB)',\n",
    "                    'Average Files per Dataset',\n",
    "                    'Unique Affiliations',\n",
    "                    'Most Common Affiliation',\n",
    "                    'Retrieval Date'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    len(df),\n",
    "                    '2025',\n",
    "                    df['license'].mode().iloc[0] if not df['license'].mode().empty else 'N/A',\n",
    "                    df['language'].mode().iloc[0] if not df['language'].mode().empty else 'N/A',\n",
    "                    round(df['total_size_bytes'].sum() / (1024**3), 2),\n",
    "                    round(df['file_count'].mean(), 2),\n",
    "                    len(affiliations_df),\n",
    "                    affiliations_df['instituce'].iloc[0] if len(affiliations_df) > 0 else 'N/A',\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        print(f\"Successfully saved {len(processed_records)} records to all formats\")\n",
    "        print(f\"Files created: {csv_filename}, {xlsx_filename}, {json_filename}, {affiliations_filename}\")\n",
    "        print(f\"Found {len(affiliations_df)} unique affiliations\")\n",
    "        \n",
    "        return csv_filename, xlsx_filename, json_filename, affiliations_filename, df, affiliations_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the Zenodo dataset retrieval\n",
    "    \"\"\"\n",
    "    print(\"Starting Zenodo Dataset Retrieval for 2025...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    retriever = ZenodoDatasetRetriever()\n",
    "    \n",
    "    # Retrieve all dataset records from 2025\n",
    "    records = retriever.get_datasets_2025()\n",
    "    \n",
    "    if not records:\n",
    "        print(\"No records found for 2025\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nTotal records retrieved: {len(records)}\")\n",
    "    \n",
    "    # Save to files\n",
    "    csv_file, xlsx_file, json_file, affiliations_file, df, affiliations_df = retriever.save_to_files(records)\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"SUMMARY STATISTICS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total datasets: {len(df)}\")\n",
    "    print(f\"Date range: 2025-01-01 to 2025-12-31\")\n",
    "    print(f\"Total size: {df['total_size_bytes'].sum() / (1024**3):.2f} GB\")\n",
    "    print(f\"Average files per dataset: {df['file_count'].mean():.2f}\")\n",
    "    print(f\"Most common license: {df['license'].mode().iloc[0] if not df['license'].mode().empty else 'N/A'}\")\n",
    "    \n",
    "    # Top 10 most common keywords\n",
    "    all_keywords = []\n",
    "    for keywords_str in df['keywords'].dropna():\n",
    "        if keywords_str:\n",
    "            all_keywords.extend([kw.strip() for kw in keywords_str.split(';')])\n",
    "    \n",
    "    if all_keywords:\n",
    "        keyword_counts = pd.Series(all_keywords).value_counts().head(10)\n",
    "        print(f\"\\nTop 10 keywords:\")\n",
    "        for keyword, count in keyword_counts.items():\n",
    "            print(f\"  {keyword}: {count}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 affiliations:\")\n",
    "    if len(affiliations_df) > 0:\n",
    "        for i, row in affiliations_df.head(10).iterrows():\n",
    "            print(f\"  {row['instituce']}: {row['počet_výskytů']}\")\n",
    "    else:\n",
    "        print(\"  No affiliations found\")\n",
    "    \n",
    "    print(f\"\\nFiles saved:\")\n",
    "    print(f\"  CSV: {csv_file}\")\n",
    "    print(f\"  Excel: {xlsx_file}\")\n",
    "    print(f\"  JSON: {json_file}\")\n",
    "    print(f\"  Affiliations: {affiliations_file}\")\n",
    "    \n",
    "    return df, affiliations_df\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n",
    "else:\n",
    "    # If running in Jupyter, you can call functions individually\n",
    "    print(\"Zenodo Dataset Retriever loaded. Use main() to run the full retrieval.\")\n",
    "    print(\"Or create a ZenodoDatasetRetriever instance for custom usage.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
